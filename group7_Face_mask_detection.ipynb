{"cells":[{"cell_type":"markdown","source":["### Mask Detection with Deep Learning"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e5c084d-9a23-44bf-a684-40d1a09a1155"}}},{"cell_type":"markdown","source":["### Loading necessary libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"156dae77-82ae-4d41-aed4-5d479d7e9554"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\nfrom petastorm.spark import SparkDatasetConverter, make_spark_converter\nimport io\nimport numpy as np\nimport torch\nimport torchvision\nfrom PIL import Image\nfrom functools import partial \nfrom petastorm import TransformSpec\nfrom torchvision import transforms\nfrom hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK\nimport horovod.torch as hvd\nfrom sparkdl import HorovodRunner\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport PIL\nfrom pyspark.context import *\n\nfrom time import time\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"803ca01a-6181-4e97-a740-4c8d19c864a5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:158: FutureWarning: pyarrow.LocalFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n  original_result = python_builtin_import(name, globals, locals, fromlist, level)\n/databricks/python/lib/python3.8/site-packages/petastorm/spark/spark_dataset_converter.py:28: FutureWarning: pyarrow.LocalFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n  from pyarrow import LocalFileSystem\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:158: FutureWarning: pyarrow.LocalFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n  original_result = python_builtin_import(name, globals, locals, fromlist, level)\n/databricks/python/lib/python3.8/site-packages/petastorm/spark/spark_dataset_converter.py:28: FutureWarning: pyarrow.LocalFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n  from pyarrow import LocalFileSystem\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Initiating SparkSession and setting appName as Mask Detection"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76bf8728-31e2-4d33-802b-3a4c143cdcf0"}}},{"cell_type":"code","source":["spark = SparkSession.builder \\\n            .appName(\"mask_detection\") \\\n            .getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa678bd7-823e-4509-adb3-86eda9ca8093"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Setting Spark Configuration for Spark and Connection Configuration for AWS and MongoDB"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03e2e8d1-be23-442a-a6cb-64d742ce0de7"}}},{"cell_type":"code","source":["aws_access_key = 'Insert_Access_Key'\naws_secret_key = 'Insert_Secret_Key'\nspark._jsc.hadoopConfiguration().set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.1')\nspark._jsc.hadoopConfiguration().set('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \nspark._jsc.hadoopConfiguration().set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\nspark._jsc.hadoopConfiguration().set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", aws_access_key)\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", aws_secret_key)\nspark._jsc.hadoopConfiguration().set(\"spark.executor.heartbeatInterval\",\"1200s\")\nspark._jsc.hadoopConfiguration().set(\"spark.network.timeout\",\"7200s\")\nspark._jsc.hadoopConfiguration().set(\"spark.executor.memory\",\"8g\")\nspark._jsc.hadoopConfiguration().set(\"spark.driver.memory\", \"12g\")\nspark._jsc.hadoopConfiguration().set(\"spark.executor.instances\", \"4\")\nspark._jsc.hadoopConfiguration().set(\"spark.executor.cores\", \"3\")\n\ndatabase = 'group7_db'\ncollection = 'test_images'\nuser_name = 'group7'\npassword = 'Karotan207!'\naddress = 'group7cluster.y0pkn.mongodb.net'\nconnection_string = f\"mongodb+srv://{user_name}:{password}@{address}/{database}.{collection}\"\nconnection_string"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"acc47f67-e6c0-4a7a-b6a0-6b69fae9cd3b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[3]: &#39;mongodb+srv://group7:Karotan207!@group7cluster.y0pkn.mongodb.net/group7_db.test_images&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: &#39;mongodb+srv://group7:Karotan207!@group7cluster.y0pkn.mongodb.net/group7_db.test_images&#39;</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Reading Pre-Processed Binaries Mask Dataset from MongoDB"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fdd7551e-2d8d-4645-8528-3f259402a5ae"}}},{"cell_type":"code","source":["df_mask_read = spark.read.format(\"mongo\").option(\"uri\",connection_string).load()\n#df_mask_read.cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0231a69a-a528-42c7-b57f-f6b29a3b7a6e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["### Displaying the sample records from the dataframe\ndf_mask_read.show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2691e906-859d-4874-80cf-34fb0518984b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------------------+--------------------+-----+-------+-------------------+--------------------+\n|                 _id|             content|label| length|   modificationTime|                path|\n+--------------------+--------------------+-----+-------+-------------------+--------------------+\n|{622c0cd492f00352...|[FF D8 FF E0 00 1...|    1|7438629|2022-03-08 06:27:27|s3://msds697group...|\n|{622c0cd492f00352...|[FF D8 FF E1 30 2...|    1|2860950|2022-03-08 06:33:54|s3://msds697group...|\n+--------------------+--------------------+-----+-------+-------------------+--------------------+\nonly showing top 2 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+--------------------+-----+-------+-------------------+--------------------+\n                 _id|             content|label| length|   modificationTime|                path|\n+--------------------+--------------------+-----+-------+-------------------+--------------------+\n{622c0cd492f00352...|[FF D8 FF E0 00 1...|    1|7438629|2022-03-08 06:27:27|s3://msds697group...|\n{622c0cd492f00352...|[FF D8 FF E1 30 2...|    1|2860950|2022-03-08 06:33:54|s3://msds697group...|\n+--------------------+--------------------+-----+-------+-------------------+--------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# mongo_data.repartition(1).write.parquet(\"mask_read_training\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65451d47-379b-4193-b408-6a13d0500236"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# df_mask_read = spark.read.parquet(\"/mask_read_training\")\n# df_mask_read.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81cb704d-33ea-4fed-9e9b-d2d924b761e6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Creating the train and validation dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3aff5d6c-3423-4f16-8fdf-d479e3787fd7"}}},{"cell_type":"code","source":["df = df_mask_read.select('content', 'label').withColumnRenamed('label', 'label_index') #.sampleBy(\"label_index\", {0: 0.1, 1: 0.1})\ndf_train, df_val = df.randomSplit([0.8, 0.2], seed=7)\nnum_classes = 2\n# Make sure the number of partitions is at least the number of workers which is required for distributed training.\ndf_train = df_train.repartition(2)\ndf_val = df_val.repartition(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32d25866-e48e-48e9-9499-207484cdae29"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Caching training and validation dataset using petaStorm"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20248f4b-9cc0-4320-8808-9b8d564aadab"}}},{"cell_type":"code","source":["# Set a cache directory on DBFS FUSE for intermediate data.\nspark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, \"file:///dbfs/tmp/petastorm/cache5\")\n\nconverter_train = make_spark_converter(df_train)\nconverter_val = make_spark_converter(df_val)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a22af98-3cfb-4522-88fc-a2acbcb1c0fd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/databricks/python/lib/python3.8/site-packages/petastorm/fs_utils.py:88: FutureWarning: pyarrow.localfs is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n  self._filesystem = pyarrow.localfs\nConverting floating-point columns to float32\nConverting floating-point columns to float32\nThe median size 18338606 B (&lt; 50 MB) of the parquet files is too small. Total size: 35967745 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:/dbfs/tmp/petastorm/cache5/20220313025102-appid-app-20220313024158-0000-9bcabbd3-6f5a-4c99-8dc5-56115c7d7bfb/part-00000-tid-5187657649456025804-425687ad-37c7-41d1-805e-77bafd1f1452-16-1-c000.parquet, ...\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python/lib/python3.8/site-packages/petastorm/fs_utils.py:88: FutureWarning: pyarrow.localfs is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n  self._filesystem = pyarrow.localfs\nConverting floating-point columns to float32\nConverting floating-point columns to float32\nThe median size 18338606 B (&lt; 50 MB) of the parquet files is too small. Total size: 35967745 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:/dbfs/tmp/petastorm/cache5/20220313025102-appid-app-20220313024158-0000-9bcabbd3-6f5a-4c99-8dc5-56115c7d7bfb/part-00000-tid-5187657649456025804-425687ad-37c7-41d1-805e-77bafd1f1452-16-1-c000.parquet, ...\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["### Checking the length of petaStorm converter train and validation dataset\nprint(f\"train: {len(converter_train)}, val: {len(converter_val)}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d90c014a-81ac-422f-81a3-c79e442a7387"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">train: 2923, val: 723\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">train: 2923, val: 723\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Creating a generic get_model function for different deep learning models"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f060b53-a02e-4f76-ab7d-59ee5723b193"}}},{"cell_type":"code","source":["def get_model(model, lr=0.001):\n    # Load a model\n\n    # Freeze parameters in the feature extraction layers\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Add a new classifier layer for transfer learning\n    if 'classifier' in dir(model):\n        num_ftrs = model.classifier[-1].in_features\n        model.classifier[-1] = torch.nn.Linear(num_ftrs, 2)\n    else:\n        num_ftrs = model.fc.in_features\n        model.fc = torch.nn.Linear(num_ftrs, 2)\n    #num_ftrs = model.classifier[1].in_features\n    # Parameters of newly constructed modules have requires_grad=True by default\n\n    return model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2f03995-9d71-46e4-8e48-cf2ef5035d7d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Defining function for training one epoch"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e64dd4ac-ff28-4603-804a-08a1087c7a8a"}}},{"cell_type":"code","source":["def train_one_epoch(model, criterion, optimizer, scheduler, train_dataloader_iter, steps_per_epoch, epoch, device):\n    \n    model.train()  # Set model to training mode\n\n    # statistics\n    running_loss = 0.0\n    running_corrects = 0\n\n    # Iterate over the data for one epoch.\n    for step in range(steps_per_epoch):\n        pd_batch = next(train_dataloader_iter)\n        inputs, labels = pd_batch['features'].to(device), pd_batch['label_index'].to(device)\n\n        # Track history in training\n        with torch.set_grad_enabled(True):\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            outputs = outputs.squeeze(1)\n            labels = labels.type(torch.LongTensor)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n        # statistics\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n  \n    scheduler.step()\n\n    epoch_loss = running_loss / (steps_per_epoch * BATCH_SIZE)\n    epoch_acc = running_corrects.double() / (steps_per_epoch * BATCH_SIZE)\n\n    print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n    return epoch_loss, epoch_acc\n\ndef evaluate(model, criterion, val_dataloader_iter, validation_steps, device, metric_agg_fn=None):\n    model.eval()  # Set model to evaluate mode\n\n    # statistics\n    running_loss = 0.0\n    running_corrects = 0\n\n    # Iterate over all the validation data.\n    for step in range(validation_steps):\n        pd_batch = next(val_dataloader_iter)\n        inputs, labels = pd_batch['features'].to(device), pd_batch['label_index'].to(device)\n\n        # Do not track history in evaluation to save memory\n        with torch.set_grad_enabled(False):\n            # forward\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            outputs = outputs.squeeze(1)\n            labels = labels.type(torch.LongTensor)\n            loss = criterion(outputs, labels)\n\n        # statistics\n        running_loss += loss.item()\n        running_corrects += torch.sum(preds == labels.data)\n\n    # The losses are averaged across observations for each minibatch.\n    epoch_loss = running_loss / validation_steps\n    epoch_acc = running_corrects.double() / (validation_steps * BATCH_SIZE)\n  \n    # metric_agg_fn is used in the distributed training to aggregate the metrics on all workers\n    if metric_agg_fn is not None:\n        epoch_loss = metric_agg_fn(epoch_loss, 'avg_loss')\n        epoch_acc = metric_agg_fn(epoch_acc, 'avg_acc')\n\n    print('Validation Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n    return epoch_loss, epoch_acc"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2766f15-8cb2-44bd-b581-d6521d214481"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Applying Transformation to input train and validation images"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df5d7719-7147-4ef8-becc-0dce030a19c9"}}},{"cell_type":"code","source":["def transform_row(is_train, pd_batch):\n  \"\"\"\n  The input and output of this function must be pandas dataframes.\n  Do data augmentation for the training dataset only.\n  \"\"\"\n  transformers = [transforms.Lambda(lambda x: Image.open(io.BytesIO(x)).convert('RGB'))]\n  #transformers = [transforms.Lambda(lambda x: Image.fromarray(x))]\n  if is_train:\n    transformers.extend([\n      transforms.RandomResizedCrop(224),\n      transforms.RandomHorizontalFlip(),\n    ])\n  else:\n    transformers.extend([\n      transforms.Resize(256),\n      transforms.CenterCrop(224),\n    ])\n  transformers.extend([\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n  ])\n  \n  trans = transforms.Compose(transformers)\n  #print (pd_batch)\n  \n  pd_batch['features'] = pd_batch['content'].map(lambda x: trans(x).numpy())\n  #print (pd_batch)\n  pd_batch = pd_batch.drop(labels=['content'], axis=1)\n  return pd_batch\n\ndef get_transform_spec(is_train=True):\n  # Note that the output shape of the `TransformSpec` is not automatically known by petastorm, \n  # so we need to specify the shape for new columns in `edit_fields` and specify the order of \n  # the output columns in `selected_fields`.\n  return TransformSpec(partial(transform_row, is_train), \n                       edit_fields=[('features', np.float32, (3, 224, 224), False)], \n                       selected_fields=['features', 'label_index'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc9e523a-d95a-4c1b-a349-051ebf1f2e34"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Global Variables for BATCH_SIZE and NUM_EPOCHS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45f1f915-f91e-4d1c-b226-6fa8beaed8bb"}}},{"cell_type":"code","source":["BATCH_SIZE = 256\nNUM_EPOCHS = 10"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ed9f5b8-5dc1-4da6-88e1-de92a5021e39"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Creating Train and Evaluate Function"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe34cb5e-dca6-4bf1-b93b-1e6d06f2ae5d"}}},{"cell_type":"code","source":["def train_and_evaluate(model, lr=1e-6):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    device = 'cpu'\n    model = get_model(model, lr=lr)\n    model = model.to(device)\n\n    criterion = torch.nn.functional.cross_entropy #torch.nn.functional.binary_cross_entropy_with_logits#torch.nn.CrossEntropyLoss()\n\n    # Only parameters of final layer are being optimized.\n    optimizer = torch.optim.AdamW(model.classifier[-1].parameters(), lr=lr)\n\n    # Decay LR by a factor of 0.1 every 7 epochs\n    exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\n    with converter_train.make_torch_dataloader(transform_spec=get_transform_spec(is_train=True), batch_size=BATCH_SIZE) as train_dataloader,converter_val.make_torch_dataloader(transform_spec=get_transform_spec(is_train=False), batch_size=BATCH_SIZE) as val_dataloader:\n\n        train_dataloader_iter = iter(train_dataloader)\n        steps_per_epoch = len(converter_train) // BATCH_SIZE\n\n        val_dataloader_iter = iter(val_dataloader)\n        #print (len(converter_val), BATCH_SIZE, len(converter_val))\n        validation_steps = (len(converter_val) // BATCH_SIZE)\n    \n        for epoch in range(NUM_EPOCHS):\n            print('Epoch {}/{}'.format(epoch + 1, NUM_EPOCHS))\n            print('-' * 10)\n\n            train_loss, train_acc = train_one_epoch(model, criterion, optimizer, exp_lr_scheduler, train_dataloader_iter, steps_per_epoch, epoch, device)\n            val_loss, val_acc = evaluate(model, criterion, val_dataloader_iter, validation_steps, device)\n\n    return val_loss\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4556e1a-3e31-4eb8-baa1-dfea4e20a4fd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Exploring different Classification Architecture\n1. MobileNetV2\n2. MobileNetV3_small\n3. MnasNet\n4. EfficientNet"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7559f63-332a-463e-afa9-8ec6c8702849"}}},{"cell_type":"markdown","source":["### Model 1: MobileNetV2 \n##### Abdus Khan"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6f21d6b-4f46-4042-8eb0-c6b3cc051c0e"}}},{"cell_type":"code","source":["tick = time()\nmodel = torchvision.models.mobilenet_v2(pretrained=True)\nloss = train_and_evaluate(model, lr=1e-6)\nprint(f'Time taken: {time() - tick} seconds')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca42e37f-6148-4f7a-af2f-64ebaba71591"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Epoch 1/10\n----------\nTrain Loss: 0.7170 Acc: 0.4904\nValidation Loss: 0.7355 Acc: 0.4414\nEpoch 2/10\n----------\nTrain Loss: 0.7243 Acc: 0.4947\nValidation Loss: 0.6992 Acc: 0.5469\nEpoch 3/10\n----------\nTrain Loss: 0.7027 Acc: 0.5330\nValidation Loss: 0.7259 Acc: 0.4941\nEpoch 4/10\n----------\nTrain Loss: 0.7307 Acc: 0.4819\nValidation Loss: 0.7166 Acc: 0.5137\nEpoch 5/10\n----------\nTrain Loss: 0.7034 Acc: 0.5263\nValidation Loss: 0.6905 Acc: 0.5625\nEpoch 6/10\n----------\nTrain Loss: 0.7244 Acc: 0.4858\nValidation Loss: 0.7371 Acc: 0.4746\nEpoch 7/10\n----------\nTrain Loss: 0.7108 Acc: 0.5128\nValidation Loss: 0.7059 Acc: 0.5332\nEpoch 8/10\n----------\nTrain Loss: 0.7373 Acc: 0.4688\nValidation Loss: 0.7031 Acc: 0.5410\nEpoch 9/10\n----------\nTrain Loss: 0.6904 Acc: 0.5430\nValidation Loss: 0.7328 Acc: 0.4902\nEpoch 10/10\n----------\nTrain Loss: 0.7267 Acc: 0.4776\nValidation Loss: 0.6982 Acc: 0.5547\nTime taken: 2302.936623573303 seconds\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Epoch 1/10\n----------\nTrain Loss: 0.7170 Acc: 0.4904\nValidation Loss: 0.7355 Acc: 0.4414\nEpoch 2/10\n----------\nTrain Loss: 0.7243 Acc: 0.4947\nValidation Loss: 0.6992 Acc: 0.5469\nEpoch 3/10\n----------\nTrain Loss: 0.7027 Acc: 0.5330\nValidation Loss: 0.7259 Acc: 0.4941\nEpoch 4/10\n----------\nTrain Loss: 0.7307 Acc: 0.4819\nValidation Loss: 0.7166 Acc: 0.5137\nEpoch 5/10\n----------\nTrain Loss: 0.7034 Acc: 0.5263\nValidation Loss: 0.6905 Acc: 0.5625\nEpoch 6/10\n----------\nTrain Loss: 0.7244 Acc: 0.4858\nValidation Loss: 0.7371 Acc: 0.4746\nEpoch 7/10\n----------\nTrain Loss: 0.7108 Acc: 0.5128\nValidation Loss: 0.7059 Acc: 0.5332\nEpoch 8/10\n----------\nTrain Loss: 0.7373 Acc: 0.4688\nValidation Loss: 0.7031 Acc: 0.5410\nEpoch 9/10\n----------\nTrain Loss: 0.6904 Acc: 0.5430\nValidation Loss: 0.7328 Acc: 0.4902\nEpoch 10/10\n----------\nTrain Loss: 0.7267 Acc: 0.4776\nValidation Loss: 0.6982 Acc: 0.5547\nTime taken: 2302.936623573303 seconds\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# def train_fn(lr):\n#     loss = train_and_evaluate(lr)\n#     return {'loss': loss, 'status': STATUS_OK}\n\n# search_space = hp.loguniform('lr', -10, -4)\n\n# argmin = fmin(\n#   fn=train_fn,\\\n#   space=search_space,\\\n#   algo=tpe.suggest,\\\n#   max_evals=2,\\\n#   trials=SparkTrials(parallelism=2))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0ccf233-1769-48b4-8ecf-fc1ebde218f7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Hyperopt with SparkTrials will automatically track trials in MLflow. To view the MLflow experiment associated with the notebook, click the &#39;Runs&#39; icon in the notebook context bar on the upper right. There, you can view all runs.\nTo view logs from trials, please check the Spark executor logs. To view executor logs, expand &#39;Spark Jobs&#39; above until you see the (i) icon next to the stage from the trial job. Click it and find the list of tasks. Click the &#39;stderr&#39; link for a task to view trial logs.\n\r  0%|          | 0/2 [00:00&lt;?, ?trial/s, best loss=?]\r                                                     \r/databricks/spark/python/pyspark/rdd.py:980: FutureWarning: Deprecated in 3.1, Use pyspark.InheritableThread with the pinned thread mode enabled.\n  warnings.warn(\n\n\r  0%|          | 0/2 [00:01&lt;?, ?trial/s, best loss=?]\r 50%|█████     | 1/2 [03:55&lt;03:55, 235.33s/trial, best loss: 0.7440483868122101]\r100%|██████████| 2/2 [07:48&lt;00:00, 234.12s/trial, best loss: 0.7138401567935944]\r100%|██████████| 2/2 [07:48&lt;00:00, 234.30s/trial, best loss: 0.7138401567935944]\nTotal Trials: 2: 2 succeeded, 0 failed, 0 cancelled.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Hyperopt with SparkTrials will automatically track trials in MLflow. To view the MLflow experiment associated with the notebook, click the &#39;Runs&#39; icon in the notebook context bar on the upper right. There, you can view all runs.\nTo view logs from trials, please check the Spark executor logs. To view executor logs, expand &#39;Spark Jobs&#39; above until you see the (i) icon next to the stage from the trial job. Click it and find the list of tasks. Click the &#39;stderr&#39; link for a task to view trial logs.\n\r  0%|          | 0/2 [00:00&lt;?, ?trial/s, best loss=?]\r                                                     \r/databricks/spark/python/pyspark/rdd.py:980: FutureWarning: Deprecated in 3.1, Use pyspark.InheritableThread with the pinned thread mode enabled.\n  warnings.warn(\n\n\r  0%|          | 0/2 [00:01&lt;?, ?trial/s, best loss=?]\r 50%|█████     | 1/2 [03:55&lt;03:55, 235.33s/trial, best loss: 0.7440483868122101]\r100%|██████████| 2/2 [07:48&lt;00:00, 234.12s/trial, best loss: 0.7138401567935944]\r100%|██████████| 2/2 [07:48&lt;00:00, 234.30s/trial, best loss: 0.7138401567935944]\nTotal Trials: 2: 2 succeeded, 0 failed, 0 cancelled.\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Model 2: MobileNetV3_small \n##### Ankush Gupta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3eb253c7-5d71-408e-8265-d5a70f49ae97"}}},{"cell_type":"code","source":["BATCH_SIZE = 256\nNUM_EPOCHS = 5\ntick = time()\nmodel = torchvision.models.mobilenet_v3_small(pretrained=True)\nloss = train_and_evaluate(model, lr=1e-6)\nprint(f'Time taken: {time() - tick} seconds')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"baaff439-4e72-4903-9969-799b3bc2cc51"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Epoch 1/5\n----------\nTrain Loss: 0.7183 Acc: 0.4947\nValidation Loss: 0.6974 Acc: 0.5215\nEpoch 2/5\n----------\nTrain Loss: 0.7183 Acc: 0.5046\nValidation Loss: 0.7149 Acc: 0.5000\nEpoch 3/5\n----------\nTrain Loss: 0.7218 Acc: 0.4883\nValidation Loss: 0.7267 Acc: 0.4746\nEpoch 4/5\n----------\nTrain Loss: 0.7215 Acc: 0.4957\nValidation Loss: 0.7037 Acc: 0.5371\nEpoch 5/5\n----------\nTrain Loss: 0.7236 Acc: 0.4922\nValidation Loss: 0.7342 Acc: 0.4746\nTime taken: 321.72924518585205 seconds\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Epoch 1/5\n----------\nTrain Loss: 0.7183 Acc: 0.4947\nValidation Loss: 0.6974 Acc: 0.5215\nEpoch 2/5\n----------\nTrain Loss: 0.7183 Acc: 0.5046\nValidation Loss: 0.7149 Acc: 0.5000\nEpoch 3/5\n----------\nTrain Loss: 0.7218 Acc: 0.4883\nValidation Loss: 0.7267 Acc: 0.4746\nEpoch 4/5\n----------\nTrain Loss: 0.7215 Acc: 0.4957\nValidation Loss: 0.7037 Acc: 0.5371\nEpoch 5/5\n----------\nTrain Loss: 0.7236 Acc: 0.4922\nValidation Loss: 0.7342 Acc: 0.4746\nTime taken: 321.72924518585205 seconds\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Model 3: MnasNet1_0 \n##### Ronica Gupta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccde6868-5709-4df1-96ef-e5aef52c1557"}}},{"cell_type":"code","source":["BATCH_SIZE = 256\nNUM_EPOCHS = 5\ntick = time()\nmodel = torchvision.models.mnasnet1_0(pretrained=True)\nloss = train_and_evaluate(model, lr=1e-6)\nprint(f'Time taken: {time() - tick} seconds')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a234876-f51d-48a9-94c8-e9d04433fbbf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Epoch 1/10\n----------\nTrain Loss: 0.7070 Acc: 0.4947\nValidation Loss: 0.6846 Acc: 0.5801\nEpoch 2/10\n----------\nTrain Loss: 0.7105 Acc: 0.4925\nValidation Loss: 0.7217 Acc: 0.4414\nEpoch 3/10\n----------\nTrain Loss: 0.7024 Acc: 0.5053\nValidation Loss: 0.6961 Acc: 0.5312\nEpoch 4/10\n----------\nTrain Loss: 0.7048 Acc: 0.5135\nValidation Loss: 0.7052 Acc: 0.4961\nEpoch 5/10\n----------\nTrain Loss: 0.7044 Acc: 0.4940\nValidation Loss: 0.7018 Acc: 0.5078\nEpoch 6/10\n----------\nTrain Loss: 0.7105 Acc: 0.4822\nValidation Loss: 0.6958 Acc: 0.5312\nEpoch 7/10\n----------\nTrain Loss: 0.7051 Acc: 0.5011\nValidation Loss: 0.7216 Acc: 0.4453\nEpoch 8/10\n----------\nTrain Loss: 0.7049 Acc: 0.5018\nValidation Loss: 0.6816 Acc: 0.5840\nEpoch 9/10\n----------\nTrain Loss: 0.7040 Acc: 0.5089\nValidation Loss: 0.7360 Acc: 0.4023\nEpoch 10/10\n----------\nTrain Loss: 0.6998 Acc: 0.5064\nValidation Loss: 0.6792 Acc: 0.5859\nTime taken: 1912.3635523319244 seconds\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Epoch 1/10\n----------\nTrain Loss: 0.7070 Acc: 0.4947\nValidation Loss: 0.6846 Acc: 0.5801\nEpoch 2/10\n----------\nTrain Loss: 0.7105 Acc: 0.4925\nValidation Loss: 0.7217 Acc: 0.4414\nEpoch 3/10\n----------\nTrain Loss: 0.7024 Acc: 0.5053\nValidation Loss: 0.6961 Acc: 0.5312\nEpoch 4/10\n----------\nTrain Loss: 0.7048 Acc: 0.5135\nValidation Loss: 0.7052 Acc: 0.4961\nEpoch 5/10\n----------\nTrain Loss: 0.7044 Acc: 0.4940\nValidation Loss: 0.7018 Acc: 0.5078\nEpoch 6/10\n----------\nTrain Loss: 0.7105 Acc: 0.4822\nValidation Loss: 0.6958 Acc: 0.5312\nEpoch 7/10\n----------\nTrain Loss: 0.7051 Acc: 0.5011\nValidation Loss: 0.7216 Acc: 0.4453\nEpoch 8/10\n----------\nTrain Loss: 0.7049 Acc: 0.5018\nValidation Loss: 0.6816 Acc: 0.5840\nEpoch 9/10\n----------\nTrain Loss: 0.7040 Acc: 0.5089\nValidation Loss: 0.7360 Acc: 0.4023\nEpoch 10/10\n----------\nTrain Loss: 0.6998 Acc: 0.5064\nValidation Loss: 0.6792 Acc: 0.5859\nTime taken: 1912.3635523319244 seconds\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Model 4: EfficientNet_b0 \n###### Tanjin Sharma"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1bb95a10-f26c-4e6b-abe7-7cd403116778"}}},{"cell_type":"code","source":["BATCH_SIZE = 256\nNUM_EPOCHS = 5\ntick = time()\nmodel = torchvision.models.efficientnet_b0(pretrained=True)\nloss = train_and_evaluate(model, lr=1e-6)\nprint(f'Time taken: {time() - tick} seconds')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b3b9037-9951-4adb-8e01-c9f2e728636d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/databricks/python/lib/python3.8/site-packages/petastorm/fs_utils.py:88: FutureWarning: pyarrow.localfs is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n  self._filesystem = pyarrow.localfs\nEpoch 1/5\n----------\nTrain Loss: 0.7116 Acc: 0.4883\nValidation Loss: 0.7076 Acc: 0.5176\nEpoch 2/5\n----------\nTrain Loss: 0.7120 Acc: 0.4744\nValidation Loss: 0.7037 Acc: 0.5059\nEpoch 3/5\n----------\nTrain Loss: 0.7170 Acc: 0.4709\nValidation Loss: 0.6988 Acc: 0.4980\nEpoch 4/5\n----------\nTrain Loss: 0.7107 Acc: 0.4854\nValidation Loss: 0.7171 Acc: 0.4688\nEpoch 5/5\n----------\nTrain Loss: 0.7117 Acc: 0.4812\nValidation Loss: 0.6941 Acc: 0.5449\nTime taken: 1216.0338299274445 seconds\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python/lib/python3.8/site-packages/petastorm/fs_utils.py:88: FutureWarning: pyarrow.localfs is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n  self._filesystem = pyarrow.localfs\nEpoch 1/5\n----------\nTrain Loss: 0.7116 Acc: 0.4883\nValidation Loss: 0.7076 Acc: 0.5176\nEpoch 2/5\n----------\nTrain Loss: 0.7120 Acc: 0.4744\nValidation Loss: 0.7037 Acc: 0.5059\nEpoch 3/5\n----------\nTrain Loss: 0.7170 Acc: 0.4709\nValidation Loss: 0.6988 Acc: 0.4980\nEpoch 4/5\n----------\nTrain Loss: 0.7107 Acc: 0.4854\nValidation Loss: 0.7171 Acc: 0.4688\nEpoch 5/5\n----------\nTrain Loss: 0.7117 Acc: 0.4812\nValidation Loss: 0.6941 Acc: 0.5449\nTime taken: 1216.0338299274445 seconds\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##### While Training large models, we were facing memory issues, that's why we chose models which have comparatively lesser parameters"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7bb1467-1c7b-47a5-9d84-51f2da63a9a0"}}},{"cell_type":"markdown","source":["### References\n1. <a href= \"https://arxiv.org/pdf/1801.04381.pdf\"> MobileNetV2 </a>\n2. <a href= \"https://arxiv.org/pdf/1905.02244.pdf\"> MobileNetV3 </a>\n3. <a href= \"https://arxiv.org/pdf/1807.11626.pdf\"> MnasNet </a>\n4. <a href= \"https://github.com/uber/petastorm\"> PetaStorm </a>\n5. <a href= \"https://spark.apache.org\"> Apache Spark </a>\n6. <a href= \"https://www.mongodb.com/\"> MongoDB </a>\n7. <a href= \"https://aws.amazon.com\"> Amazon AWS </a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa88ce0a-831e-4bbe-a8ef-b0a53ba5780a"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Face_mask_detection","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3164413804760689}},"nbformat":4,"nbformat_minor":0}
